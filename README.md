
The next are estimations of training some models


| Model          | Training Tokens (30 days, 96 GPUs H200) | Datasize Processed in 30 Days (TB) |
|----------------|----------------------------------------|-------------------------------|
| GPT3-5B        | 7.69T                                 | 15.015                        |
| GPT3-20B       | 1.92T                                 | 3.754                         |
| GPT3-175B      | 235.18B                               | 0.459                         |
| GPT3-175B      | 270.98B                               | 0.529                         |
| LLAMA2-7B      | 5.56T                                 | 10.863                        |
| LLAMA2-13B     | 2.86T                                 | 5.591                         |
| LLAMA2-70B     | 567.58B                               | 1.109                         |
| Nemotron-8B    | 4.11T                                 | 8.023                         |
| Nemotron-22B   | 1.42T                                 | 2.766                         |
| Nemotron-340B  | 107.08B                               | 0.209                         |
| LLAMA3-8B      | 4.03T                                 | 7.873                         |
| LLAMA3-70B     | 500.57B                               | 0.978                         |
