{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Time Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training performance on H100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>#-GPUs</th>\n",
       "      <th>GBS</th>\n",
       "      <th>MBS</th>\n",
       "      <th>Sequence Length</th>\n",
       "      <th>TP</th>\n",
       "      <th>PP</th>\n",
       "      <th>CP</th>\n",
       "      <th>VP</th>\n",
       "      <th>Tokens / sec / GPU</th>\n",
       "      <th>Model TFLOP / sec / GPU</th>\n",
       "      <th>Est. time to train in days (10T tokens, 1K GPUs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT3-5B</td>\n",
       "      <td>64</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23406</td>\n",
       "      <td>765</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT3-20B</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5851</td>\n",
       "      <td>750</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT3-175B</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>716</td>\n",
       "      <td>771</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPT3-175B</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>2</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>825</td>\n",
       "      <td>888</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLAMA2-7B</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16934</td>\n",
       "      <td>780</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLAMA2-13B</td>\n",
       "      <td>16</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8715</td>\n",
       "      <td>760</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLAMA2-70B</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1728</td>\n",
       "      <td>768</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nemotron-8B</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12507</td>\n",
       "      <td>643</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nemotron-22B</td>\n",
       "      <td>64</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>4096</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4312</td>\n",
       "      <td>562</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nemotron-340B</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>4096</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>326</td>\n",
       "      <td>686</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LLAMA3-8B</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>8192</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12273</td>\n",
       "      <td>711</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LLAMA3-70B</td>\n",
       "      <td>64</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>8192</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1524</td>\n",
       "      <td>734</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  #-GPUs   GBS  MBS  Sequence Length  TP  PP  CP  VP  \\\n",
       "0         GPT3-5B      64  2048    4             2048   1   1   1   1   \n",
       "1        GPT3-20B      64   256    2             2048   2   1   1   1   \n",
       "2       GPT3-175B     128   256    1             2048   4   8   1   6   \n",
       "3       GPT3-175B     512  2048    2             2048   4   8   1   6   \n",
       "4       LLAMA2-7B       8   128    1             4096   1   1   1   1   \n",
       "5      LLAMA2-13B      16   128    1             4096   1   4   1  10   \n",
       "6      LLAMA2-70B      64   128    1             4096   4   4   1  20   \n",
       "7     Nemotron-8B      64   256    4             4096   2   1   1   1   \n",
       "8    Nemotron-22B      64   256    2             4096   2   4   1  10   \n",
       "9   Nemotron-340B     128    32    1             4096   8   8   1  12   \n",
       "10      LLAMA3-8B       8   128    1             8192   1   1   2   1   \n",
       "11     LLAMA3-70B      64   128    1             8192   4   4   2   5   \n",
       "\n",
       "    Tokens / sec / GPU  Model TFLOP / sec / GPU  \\\n",
       "0                23406                      765   \n",
       "1                 5851                      750   \n",
       "2                  716                      771   \n",
       "3                  825                      888   \n",
       "4                16934                      780   \n",
       "5                 8715                      760   \n",
       "6                 1728                      768   \n",
       "7                12507                      643   \n",
       "8                 4312                      562   \n",
       "9                  326                      686   \n",
       "10               12273                      711   \n",
       "11                1524                      734   \n",
       "\n",
       "    Est. time to train in days (10T tokens, 1K GPUs)  \n",
       "0                                                  5  \n",
       "1                                                 19  \n",
       "2                                                158  \n",
       "3                                                137  \n",
       "4                                                  7  \n",
       "5                                                 13  \n",
       "6                                                 65  \n",
       "7                                                  9  \n",
       "8                                                 26  \n",
       "9                                                347  \n",
       "10                                                 9  \n",
       "11                                                74  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creando la tabla en un DataFrame\n",
    "data = {\n",
    "    \"Model\": [\"GPT3-5B\", \"GPT3-20B\", \"GPT3-175B\", \"GPT3-175B\", \"LLAMA2-7B\", \"LLAMA2-13B\", \"LLAMA2-70B\", \"Nemotron-8B\", \"Nemotron-22B\", \"Nemotron-340B\", \"LLAMA3-8B\", \"LLAMA3-70B\"],\n",
    "    \"#-GPUs\": [64, 64, 128, 512, 8, 16, 64, 64, 64, 128, 8, 64],\n",
    "    \"GBS\": [2048, 256, 256, 2048, 128, 128, 128, 256, 256, 32, 128, 128],\n",
    "    \"MBS\": [4, 2, 1, 2, 1, 1, 1, 4, 2, 1, 1, 1],\n",
    "    \"Sequence Length\": [2048, 2048, 2048, 2048, 4096, 4096, 4096, 4096, 4096, 4096, 8192, 8192],\n",
    "    \"TP\": [1, 2, 4, 4, 1, 1, 4, 2, 2, 8, 1, 4],\n",
    "    \"PP\": [1, 1, 8, 8, 1, 4, 4, 1, 4, 8, 1, 4],\n",
    "    \"CP\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2],\n",
    "    \"VP\": [1, 1, 6, 6, 1, 10, 20, 1, 10, 12, 1, 5],\n",
    "    \"Tokens / sec / GPU\": [23406, 5851, 716, 825, 16934, 8715, 1728, 12507, 4312, 326, 12273, 1524],\n",
    "    \"Model TFLOP / sec / GPU\": [765, 750, 771, 888, 780, 760, 768, 643, 562, 686, 711, 734],\n",
    "    \"Est. time to train in days (10T tokens, 1K GPUs)\": [5, 19, 158, 137, 7, 13, 65, 9, 26, 347, 9, 74]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZERO_DICT = {\n",
    "    'T': 12,  # Trillion\n",
    "    'B': 9,   # Billion\n",
    "    'M': 6,   # Million\n",
    "    'K': 3,   # Thousand\n",
    "    '': 0     # Base case (less than 1,000)\n",
    "}\n",
    "\n",
    "def format_numbres(num):\n",
    "    for suffix, zeros in ZERO_DICT.items():\n",
    "        if abs(num) >= 10 ** zeros:\n",
    "            value = num / (10 ** zeros)\n",
    "            return f\"{value:.0f}e{zeros} ({suffix})\"\n",
    "    return f\"{num:.0f}e0\"\n",
    "\n",
    "def training_time(\n",
    "    tokens_per_second: int = 23406,\n",
    "    model_name: str = \"GPT3-5B\",\n",
    "    n_gpus: int = 1000,\n",
    "    n_tokens: int = 10_000_000_000_000,\n",
    "    time_format: str = \"D\",\n",
    "):\n",
    "    total_time = n_tokens / (tokens_per_second * n_gpus)\n",
    "    if time_format == \"D\":\n",
    "        return round(total_time / (24 * 60 * 60))\n",
    "\n",
    "def training_tokens(tokens_per_second: int = 23406,\n",
    "    model_name: str = \"GPT3-5B\", n_gpus: int=96, n_days: int=30):\n",
    "    \"\"\" Calculate the amount of tokens that a fixed numbers of H200 gpu's can process\"\"\"\n",
    "    h200_tps = tokens_per_second * 1.32\n",
    "    t_tokens = (n_days * 24 * 60 * 60)* (h200_tps * n_gpus)\n",
    "    return t_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_data_size(tokens, bytes_per_token=2):\n",
    "\n",
    "    total_bytes = tokens * bytes_per_token\n",
    "    \n",
    "    size_in_gb = total_bytes / 1e9  # 1 GB = 1e9 bytes\n",
    "    size_in_tb = size_in_gb / 1024  # 1 TB = 1024 GB\n",
    "\n",
    "    return size_in_tb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Model,\"Training Tokens (30 days, 96 GPUs H200)\",Datasize processed in 30 days\\nGPT3-5B,7687893565440.001,15.015417120000002\\nGPT3-20B,1921809162240.0002,3.7535335200000004\\nGPT3-175B,235176099840.0,0.45932832\\nGPT3-175B,270978048000.0,0.529254\\nLLAMA2-7B,5562111836160.0,10.86349968\\nLLAMA2-13B,2862513561600.0,5.5908468\\nLLAMA2-70B,567575838720.0,1.10854656\\nNemotron-8B,4108027207680.0,8.02349064\\nNemotron-22B,1416311930880.0,2.76623424\\nNemotron-340B,107077386240.0,0.20913552\\nLLAMA3-8B,4031167979520.0,7.87337496\\nLLAMA3-70B,500570357760.0,0.97767648\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new columns for training time and tokens\n",
    "df['Training Days (10T tokens, 1K GPUs)'] = df.apply(lambda row: training_time(row['Tokens / sec / GPU'], row['Model']), axis=1)\n",
    "df['Training Tokens (30 days, 96 GPUs H200)'] = df.apply(lambda row: training_tokens(row['Tokens / sec / GPU'], row['Model']), axis=1)\n",
    "df['Datasize processed in 30 days'] = df.apply(lambda row: calculate_data_size(row['Training Tokens (30 days, 96 GPUs H200)']), axis=1)\n",
    "\n",
    "# Display the updated dataframe\n",
    "df[['Model','Training Tokens (30 days, 96 GPUs H200)','Datasize processed in 30 days']].to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tt_estimation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
